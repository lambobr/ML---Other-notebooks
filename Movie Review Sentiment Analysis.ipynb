{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Sentiment Analysis using Logistic Regression\n### In this case study, we will use the dataset from imbd to create a model which will classify if a movie review is a positive or negative sentiment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the necessary libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the data\n\ndf = pd.read_csv('/kaggle/input/imdb_movie_data.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('*'*10 + 'First 10 rows' + '*'*10)\nprint(df.head(10))\nprint(\"\")\nprint('*'*10 + 'Information' + '*'*10)\nprint(df.info())\nprint(\"\")\nprint('*'*10 + 'Null values' + '*'*10)\nprint(df.isnull().any())\nprint(\"\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We see that there are 50,000 rows and our data has no null values. Next let us check if there are duplicate and missing values.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('*'*10 + 'Duplicate values' + '*'*10)\nprint(df.duplicated(subset='review').value_counts())\n\nsns.heatmap(df.isnull(),cmap='viridis',cbar=False,yticklabels=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing values will appear as yellow line in the plot. So this means there are no missing values in our data.\n### Also there are 418 duplicate values, so let us drop these.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop_duplicates(subset='review', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We also need to check the distribution of the dependent variable to determine the scoring method to use.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(df.sentiment,kde=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We see that there is 50-50 distribution of our sentiment classes, therefore we can use accuracy scoring in our model.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"# 2. Cleaning the Data","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us check one data row\n\ndf.loc[0,'review']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### We see that our data contains html objects so we need to remove these.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def cleaner(text):\n    # Remove html objects\n    text = re.sub('<[^<]*>','',text)\n    \n    # Temporarily store emoticons\n    emoticons = ''.join(re.findall('[:;=]-+[\\)\\(pPD]+',text))\n    \n    # Remove non-word characters and combine back the emoticons\n    text = re.sub('\\W+',' ',text.lower()) + emoticons.replace('-','')\n    \n    return text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us check the function if it works\n\ncleaner(df.loc[0,'review'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply the function to whole dataset\n\ndf['review'] = df['review'].apply(cleaner)\ndf.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Tokenization\n\n### We use the nltk library to tokenize the documents.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"porter = PorterStemmer()\n\ndef token_porter(text):\n    return [porter.stem(word) for word in text.split()]\n\n# We will also tokenize without porter\ndef token(text):\n    return text.split()\n\n# We will pass the 2 functions in our GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Transform into feature vectors and Data splitting\n### We will use the TfidfVectorizer to transform the words into numbers and give weights to each word.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf = TfidfVectorizer(lowercase=False)\n\n# Also load the stopwords from nltk library\nstop = stopwords.words('english')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df.iloc[:,0].to_numpy()\ny = df.iloc[:,1].to_numpy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, stratify=y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Classification using LogisticRegression and GridSearchCV","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initialize parameters\nparam_grid = [{'vect__stop_words':[stop, None],\n               'vect__tokenizer':[token, token_porter],\n               'clf__penalty':['l2'],\n               'clf__C':[1, 10, 100]},\n              {'vect__use_idf':[False],\n               'vect__stop_words':[stop, None], \n               'vect__tokenizer':[token, token_porter],\n               'clf__penalty':['l2'],\n               'clf__C':[1, 10, 100]}\n             ]\n\n# Use pipeline to build composite estimator\nlr_tfidf = Pipeline([('vect', tfidf),\n                     ('clf', LogisticRegression(tol=0.01, random_state=0))])\n\ngs = GridSearchCV(lr_tfidf, \n                  param_grid, \n                  scoring='accuracy',\n                  cv=5,\n                  n_jobs=1,\n                  verbose=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fit our model to the train dataset\ngs.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 6. Model Accuracy","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Best parameter settings: %s' % gs.best_params_)\nprint('CV Accuracy:%.3f' % gs.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Get our best classifier settings\nclf = gs.best_estimator_\n\nprint('Test Accuracy: %.3f' % clf.score(X_test, y_test))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}